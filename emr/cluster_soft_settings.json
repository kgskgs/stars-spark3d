[
   {
      "classification":"spark-env",
      "properties":{
         "maximizeResourceAllocation":"false"
      },
      "configurations":[
         {
            "classification":"export",
            "properties":{
               "PYSPARK_PYTHON":"/usr/bin/python3"
            },
            "configurations":[
               
            ]
         }
      ]
   },
   {
      "classification":"yarn-site",
      "properties":{
         "yarn.nodemanager.resource.memory-mb":"14336",
         "yarn.nodemanager.pmem-check-enabled":"false",
         "yarn.scheduler.maximum-allocation-mb":"14336",
         "yarn.nodemanager.vmem-check-enabled":"false"
      },
      "configurations":[
         
      ]
   },
   {
      "classification":"spark-defaults",
      "properties":{
         "spark.executor.memory":"5812M",
         "spark.driver.memory":"5812M",
         "spark.yarn.scheduler.reporterThread.maxFailures":"1",
         "spark.driver.cores":"2",
         "spark.executor.heartbeatInterval":"60s",
         "spark.network.timeout":"800s",
         "spark.executor.cores":"2",
         "spark.memory.storageFraction":"0.30",
         "spark.sql.shuffle.partitions":"8",
         "spark.default.parallelism":"64",
         "spark.serializer":"org.apache.spark.serializer.KryoSerializer",
         "spark.executor.memoryOverhead":"646M",
         "spark.memory.fraction":"0.80",
         "spark.executor.extraJavaOptions":"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:InitiatingHeapOccupancyPercent=35 -XX:OnOutOfMemoryError='kill -9 %p'",
         "spark.executor.instances":"17",
         "spark.dynamicAllocation.enabled":"false",
         "spark.driver.extraJavaOptions":"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:InitiatingHeapOccupancyPercent=35 -XX:OnOutOfMemoryError='kill -9 %p'",
         "spark.cleaner.periodicGC.interval":"10min",
         "spark.cleaner.referenceTracking.blocking":"false",
         "spark.cleaner.referenceTracking.cleanCheckpoints":"true",
         "spark.sql.adaptive.enabled":"true",
         "spark.sql.execution.reuseSubquery":"true"
      },
      "configurations":[
         
      ]
   }
]